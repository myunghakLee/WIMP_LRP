{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a0fd51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:54:55.194251Z",
     "start_time": "2022-01-21T15:54:55.188318Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.data.argoverse_datamodule import ArgoverseDataModule\n",
    "from src.data.argoverse_dataset import ArgoverseDataset\n",
    "from src.data.dummy_datamodule import DummyDataModule\n",
    "from src.models.WIMP import WIMP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "db1ea2d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:32.332391Z",
     "start_time": "2022-01-21T18:27:32.319655Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {\"IFC\":True, \"add_centerline\":False, \"attention_heads\":4, \"batch_norm\":False, \"batch_size\":1, \"check_val_every_n_epoch\":3, \n",
    "          \"dataroot\":'./data/argoverse_processed_simple', \"dataset\":'argoverse', \"distributed_backend\":'ddp', \"dropout\":0.0, \n",
    "          \"early_stop_threshold\":5, \"experiment_name\":'example', \"gpus\":1, \"gradient_clipping\":True, \"graph_iter\":1, \n",
    "          \"hidden_dim\":512, \"hidden_key_generator\":True, \"hidden_transform\":False, \"input_dim\":2, \"k_value_threshold\":10, \n",
    "          \"k_values\":[6, 5, 4, 3, 2, 1], \"lr\":0.0001, \"map_features\":False, \"max_epochs\":200, \"mode\":'train', \"model_name\":'WIMP', \n",
    "          \"no_heuristic\":False, \"non_linearity\":'relu', \"num_layers\":4, \"num_mixtures\":6, \"num_nodes\":1, \"output_conv\":True, \"output_dim\":2, \n",
    "          \"output_prediction\":True, \"precision\":32, \"predict_delta\":False, \"resume_from_checkpoint\":None, \n",
    "          \"scheduler_step_size\":[60, 90, 120, 150, 180], \"seed\":None, \"segment_CL\":False, \"segment_CL_Encoder\":False, \n",
    "          \"segment_CL_Encoder_Gaussian\":False, \"segment_CL_Encoder_Gaussian_Prob\":False, \"segment_CL_Encoder_Prob\":True, \n",
    "          \"segment_CL_Gaussian_Prob\":False, \"segment_CL_Prob\":False, \"use_centerline_features\":True, \"use_oracle\":False, \"waypoint_step\":5, \n",
    "          \"weight_decay\":0.0, \"workers\":8, \"wta\":False, \"draw_image\" : False, \"remove_high_related_score\" : True, \"maximum_delete_num\" : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "207ecea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:34.591371Z",
     "start_time": "2022-01-21T18:27:34.058418Z"
    },
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "train_loader = ArgoverseDataset(args['dataroot'], mode='train', delta=args['predict_delta'],\n",
    "                              map_features_flag=args['map_features'],\n",
    "                              social_features_flag=True, heuristic=(not args['no_heuristic']),\n",
    "                              ifc=args['IFC'], is_oracle=args['use_oracle'])\n",
    "\n",
    "val_loader = ArgoverseDataset(args['dataroot'], mode='val', delta=args['predict_delta'],\n",
    "                              map_features_flag=args['map_features'],\n",
    "                              social_features_flag=True, heuristic=(not args['no_heuristic']),\n",
    "                              ifc=args['IFC'], is_oracle=args['use_oracle'])\n",
    "\n",
    "test_loader = ArgoverseDataset(args['dataroot'], mode='test', delta=args['predict_delta'],\n",
    "                              map_features_flag=args['map_features'],\n",
    "                              social_features_flag=True, heuristic=(not args['no_heuristic']),\n",
    "                              ifc=args['IFC'], is_oracle=args['use_oracle'])\n",
    "\n",
    "train_dataset = DataLoader(train_loader, batch_size=args['batch_size'], num_workers=args['workers'],\n",
    "                                pin_memory=True, collate_fn=ArgoverseDataset.collate,\n",
    "                                shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = DataLoader(val_loader, batch_size=args['batch_size'], num_workers=args['workers'],\n",
    "                                pin_memory=True, collate_fn=ArgoverseDataset.collate,\n",
    "                                shuffle=False, drop_last=False)\n",
    "\n",
    "trest_dataset = DataLoader(test_loader, batch_size=args['batch_size'], num_workers=args['workers'],\n",
    "                                pin_memory=True, collate_fn=ArgoverseDataset.collate,\n",
    "                                shuffle=False, drop_last=False)\n",
    "# dm = ArgoverseDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b37118c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:35.889346Z",
     "start_time": "2022-01-21T18:27:35.017978Z"
    }
   },
   "outputs": [],
   "source": [
    "model = WIMP(args)\n",
    "model.load_state_dict(torch.load(\"experiments/example/checkpoints/epoch=122.ckpt\")['state_dict'])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d506f11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:38.923603Z",
     "start_time": "2022-01-21T18:27:38.917874Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metric(metric_dict, ade,fde,mr,loss):\n",
    "    metric_dict[\"ade\"] += ade\n",
    "    metric_dict[\"fde\"] += fde\n",
    "    metric_dict[\"mr\"] += mr\n",
    "    metric_dict[\"loss\"] += loss\n",
    "    metric_dict[\"length\"]+=1\n",
    "\n",
    "# a = torch.FloatTensor([1])\n",
    "# get_metric(origina_model_metric, a,a,a,a)\n",
    "# origina_model_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2dae5442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:39.253603Z",
     "start_time": "2022-01-21T18:27:39.246759Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  optimizer는 선언하지 않았습니다.\n",
    "#  따라서 아마 model weight에 gradient는 계속 쌓이겠지만 저희가 중요한 것은 adjacency matrix의 graident이므로 상관 없을 것으로 예측됩니다.\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "Relu = nn.ReLU()\n",
    "\n",
    "save_foler = \"ResultsImg/\"\n",
    "\n",
    "save_XAI = save_foler + \"/XAI/\"\n",
    "save_attention = save_foler + \"/attention\"\n",
    "\n",
    "slicing = lambda a, idx: torch.cat((a[:, :idx], a[:, idx+1:]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fd1b17a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:42.147601Z",
     "start_time": "2022-01-21T18:27:39.728644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/39472 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/39472 [00:01<16:46:25,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 1/39472 [00:02<24:53:13,  2.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-0ca94d710bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                         preds, waypoint_preds, all_dist_params, att_weights, adjacency = model(input_dict[\"agent_features\"],\n\u001b[0m\u001b[1;32m    115\u001b[0m                                                     \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"social_features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/MotionPrediction/WIMP/src/models/WIMP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, agent_features, social_features, adjacency, num_agent_mask, outsteps, social_label_features, label_adjacency, classmate_forcing, labels, ifc_helpers, test, map_estimate, gt, idx, sample_next, num_predictions, am)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxy_kernel_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mlast_n_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mprediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaypoints_prediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_n_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifc_helpers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mifc_helpers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_next\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_estimate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_estimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwaypoints_prediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaypoint_predictions_tensor_encoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/MotionPrediction/WIMP/src/models/WIMP_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_input_features, last_n_predictions, hidden_decoder, outsteps, ifc_helpers, sample_next, map_estimate, mixture_num, sample_centerline)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mcurrent_input_xy_centerline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_linearity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_input_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input_xy_centerline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mcurrent_input_xy_centerline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_input_xy_centerline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input_xy_centerline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mcurrent_decoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input_xy_centerline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mcurr_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_decoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "abs_min = lambda weight : torch.argmin(abs(weight)[1:]).item()\n",
    "abs_max = lambda weight : torch.argmax(abs(weight)[1:]).item()\n",
    "simple_min = lambda weight : torch.argmin(weight[1:]).item()\n",
    "simple_max = lambda weight : torch.argmax(weight[1:]).item()\n",
    "\n",
    "\n",
    "names = [\"abs_min\", \"abs_max\", \"simple_min\",\"simple_max\"]\n",
    "for name_idx, function in enumerate([abs_min, abs_max, simple_min, simple_max]):\n",
    "    origina_model_metric  ={\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),                                                                                                                                         \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "        \"length\" : 0\n",
    "    }\n",
    "\n",
    "    DA_model_metric = [\n",
    "        {\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"length\" : 0    \n",
    "        },\n",
    "        {\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"length\" : 0    \n",
    "        },\n",
    "        {\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"length\" : 0\n",
    "        },\n",
    "    ] # 하나씩 지우면서 metric을 잴것임\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(val_dataset)):\n",
    "        input_dict, target_dict = batch[0], batch[1]\n",
    "\n",
    "        # get cuda\n",
    "        input_dict['agent_features'] = input_dict['agent_features'].cuda()\n",
    "        input_dict['social_features'] = input_dict['social_features'].cuda()\n",
    "        input_dict['social_label_features'] = input_dict['social_label_features'].cuda()\n",
    "        input_dict['adjacency'] = input_dict['adjacency'].cuda()\n",
    "        input_dict['label_adjacency'] = input_dict['label_adjacency'].cuda()\n",
    "        input_dict['num_agent_mask'] = input_dict['num_agent_mask'].cuda()\n",
    "        input_dict['ifc_helpers']['agent_oracle_centerline'] = input_dict['ifc_helpers']['agent_oracle_centerline'].cuda()\n",
    "        input_dict['ifc_helpers']['agent_oracle_centerline_lengths'] = input_dict['ifc_helpers']['agent_oracle_centerline_lengths'].cuda()\n",
    "        input_dict['ifc_helpers']['social_oracle_centerline'] = input_dict['ifc_helpers']['social_oracle_centerline'].cuda()\n",
    "        input_dict['ifc_helpers']['social_oracle_centerline_lengths'] = input_dict['ifc_helpers']['social_oracle_centerline_lengths'].cuda()\n",
    "        input_dict['ifc_helpers']['agent_oracle_centerline'] = input_dict['ifc_helpers']['agent_oracle_centerline'].cuda()\n",
    "        target_dict['agent_labels'] = target_dict['agent_labels'].cuda()\n",
    "\n",
    "\n",
    "        preds, waypoint_preds, all_dist_params, attention, adjacency = model(**input_dict)\n",
    "        loss, (ade, fde, mr) = model.eval_preds(preds, target_dict, waypoint_preds)\n",
    "        get_metric(origina_model_metric, ade,fde,mr,loss)\n",
    "\n",
    "\n",
    "        input_dict['adjacency'].requires_grad = True\n",
    "        input_dict['adjacency'].retain_grad()\n",
    "\n",
    "        adjacency.retain_grad()\n",
    "        print(torch.sum(model.decoder.value_generator.weight))\n",
    "        loss.backward()\n",
    "        print(torch.sum(model.decoder.value_generator.weight))\n",
    "        batch_preds = preds\n",
    "\n",
    "\n",
    "\n",
    "        for idx in range(args[\"batch_size\"]):\n",
    "            if args[\"draw_image\"]:\n",
    "                weight = adjacency.grad[idx][0].cpu().numpy()\n",
    "                att = attention[idx].cpu().numpy()\n",
    "                agent_features = input_dict['agent_features'][idx].cpu().numpy()\n",
    "                social_features = input_dict['social_features'][idx].cpu().numpy()\n",
    "                # target = target_dict['agent_labels'][idx].cpu().numpy()\n",
    "                preds = batch_preds[idx][:,:,:,:2][0].cpu().detach().numpy()\n",
    "                city_name = input_dict['ifc_helpers']['city'][idx]\n",
    "                rotation = input_dict['ifc_helpers']['rotation'][idx].numpy()\n",
    "                translation = input_dict['ifc_helpers']['translation'][idx].numpy()\n",
    "                XAI_utils.draw_attention(agent_features, social_features, preds, city_name, rotation, translation, \n",
    "                                               weight = copy.deepcopy(att), draw_future=True, save_fig = True, \n",
    "                                                 = save_attention + \"/\" + str(batch_idx) + \"_\" + str(idx) + \".png\")\n",
    "\n",
    "                XAI_utils.draw(agent_features, social_features, preds, city_name, rotation, translation, \n",
    "                                   weight = copy.deepcopy(weight), draw_future=True, save_fig = True, \n",
    "                                   save_name = save_XAI + \"/\" + str(batch_idx) + \"_\" + str(idx) + \".png\")\n",
    "\n",
    "            if args[\"remove_high_related_score\"]:\n",
    "                weight = adjacency.grad[idx][0]\n",
    "                for i in range(args[\"maximum_delete_num\"]):\n",
    "    #                 print(batch_idx, input_dict[\"social_features\"].shape)\n",
    "                    if len(input_dict[\"social_features\"][0] > 1):\n",
    "                        arg = function(weight)\n",
    "    #                     arg_max = torch.argmin(weight[1:]).item()\n",
    "\n",
    "                        weight = torch.cat((weight[:arg+1], weight[arg+2:]))                                                                                                                                                                                                                                                                                                                                                                             \n",
    "                        input_dict[\"social_features\"]                                        = slicing(input_dict[\"social_features\"], arg)\n",
    "                        input_dict[\"num_agent_mask\"]                                    = slicing(input_dict[\"num_agent_mask\"], arg+1)\n",
    "                        input_dict[\"ifc_helpers\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ['social_oracle_centerline']             = slicing(input_dict[\"ifc_helpers\"]['social_oracle_centerline'], arg)\n",
    "                        input_dict[\"ifc_helpers\"]['social_oracle_centerline_lengths']  = slicing(input_dict[\"ifc_helpers\"]['social_oracle_centerline_lengths'], arg)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        preds, waypoint_preds, all_dist_params, att_weights, adjacency = model(input_dict[\"agent_features\"],\n",
    "                                                    input_dict[\"social_features\"],\n",
    "                                                    None,\n",
    "                                                    input_dict[\"num_agent_mask\"],\n",
    "                                                    ifc_helpers ={\n",
    "                                                        \"social_oracle_centerline\": input_dict[\"ifc_helpers\"]['social_oracle_centerline'], \n",
    "                                                        \"social_oracle_centerline_lengths\": input_dict[\"ifc_helpers\"]['social_oracle_centerline_lengths'],\n",
    "                                                        \"agent_oracle_centerline\": input_dict[\"ifc_helpers\"][\"agent_oracle_centerline\"],\n",
    "                                                        \"agent_oracle_centerline_lengths\": input_dict[\"ifc_helpers\"][\"agent_oracle_centerline_lengths\"]\n",
    "                                                    })\n",
    "                        loss, (ade, fde, mr) = model.eval_preds(preds, target_dict, waypoint_preds)\n",
    "                        get_metric(DA_model_metric[i], ade,fde,mr,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0e4fb52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:48.965337Z",
     "start_time": "2022-01-21T18:27:48.007841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WIMP(\n",
       "  (encoder): WIMPEncoder(\n",
       "    (xy_conv_filters): ModuleList(\n",
       "      (0): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): Conv1d(2, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (xy_input_transform): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
       "    (non_linearity): ReLU()\n",
       "    (lstm_input_transform): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (lstm): LSTM(512, 512, num_layers=4, batch_first=True)\n",
       "    (waypoint_predictor): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (waypoint_lstm): LSTM(512, 512, num_layers=4, batch_first=True)\n",
       "    (cl_conv_filters): ModuleList(\n",
       "      (0): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): Conv1d(2, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (cl_input_transform): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
       "    (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "    (key_generator): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (query_generator): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (value_generator): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (gat): GraphAttentionLayer(\n",
       "    (W): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (a_1): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (a_2): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (decoder): WIMPDecoder(\n",
       "    (xy_conv_filters): ModuleList(\n",
       "      (0): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(3, 512, kernel_size=(3,), stride=(1,))\n",
       "      (2): Conv1d(3, 512, kernel_size=(5,), stride=(1,))\n",
       "    )\n",
       "    (output_transform): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
       "    (non_linearity): ReLU()\n",
       "    (lstm): LSTM(3072, 512, num_layers=4, batch_first=True)\n",
       "    (lstm_input_transform): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (predictor): Linear(in_features=512, out_features=18, bias=True)\n",
       "    (waypoint_predictor): Linear(in_features=512, out_features=12, bias=True)\n",
       "    (waypoint_lstm): LSTM(3072, 512, num_layers=4, batch_first=True)\n",
       "    (cl_conv_filters): ModuleList(\n",
       "      (0): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): Conv1d(2, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (cl_input_transform): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
       "    (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "    (key_generator): Linear(in_features=2048, out_features=3072, bias=True)\n",
       "    (query_generator): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (value_generator): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WIMP(args)\n",
    "model.load_state_dict(torch.load(\"experiments/example/checkpoints/epoch=122.ckpt\")['state_dict'])\n",
    "model.cuda()\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b8e3bece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:27:58.353809Z",
     "start_time": "2022-01-21T18:27:58.027162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4276, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.eval()\n",
    "preds, waypoint_preds, all_dist_params, att_weights, adjacency = model(**input_dict)\n",
    "loss, (ade, fde, mr) = model.eval_preds(preds, target_dict, waypoint_preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8732a5f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:23:17.272066Z",
     "start_time": "2022-01-21T18:23:17.262825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3187],\n",
       "         [ 0.0102]],\n",
       "\n",
       "        [[-0.2003],\n",
       "         [-0.0071]],\n",
       "\n",
       "        [[ 0.0173],\n",
       "         [-0.0028]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0160],\n",
       "         [-0.0020]],\n",
       "\n",
       "        [[-0.0888],\n",
       "         [-0.0026]],\n",
       "\n",
       "        [[-0.0369],\n",
       "         [-0.0020]]], device='cuda:0')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.xy_conv_filters[0].weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f1797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
