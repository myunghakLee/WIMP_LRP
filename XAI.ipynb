{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762dde98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:58:45.587729Z",
     "start_time": "2022-02-17T18:58:45.583399Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a0fd51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:59:01.775817Z",
     "start_time": "2022-02-17T18:58:49.632899Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.data.argoverse_datamodule import ArgoverseDataModule\n",
    "from src.data.argoverse_dataset import ArgoverseDataset\n",
    "from src.data.dummy_datamodule import DummyDataModule\n",
    "from src.models.WIMP import WIMP\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd8fb53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:59:22.328548Z",
     "start_time": "2022-02-17T18:59:22.008329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argoverse_processed_simple\targoverse_with_LRP.zip\tLRP_adjacency3\r\n",
      "argoverse_processed_simple.tar\tLRP_adjacency\t\tnuscene\r\n",
      "argoverse_with_LRP\t\tLRP_adjacency2\t\tnuscene_data\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1ea2d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:59:36.437220Z",
     "start_time": "2022-02-17T18:59:36.432277Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {\"IFC\":True, \"add_centerline\":False, \"attention_heads\":4, \"batch_norm\":False, \"batch_size\":1, \"check_val_every_n_epoch\":3, \n",
    "          \"dataroot\":'./data/argoverse_processed_simple/', \"dataset\":'argoverse', \"distributed_backend\":'ddp', \"dropout\":0.0, \n",
    "          \"early_stop_threshold\":5, \"experiment_name\":'example', \"gpus\":1, \"gradient_clipping\":True, \"graph_iter\":1, \n",
    "          \"hidden_dim\":512, \"hidden_key_generator\":True, \"hidden_transform\":False, \"input_dim\":2, \"k_value_threshold\":10, \n",
    "          \"k_values\":[6, 5, 4, 3, 2, 1], \"lr\":0.0001, \"map_features\":False, \"max_epochs\":200, \"mode\":'train', \"model_name\":'WIMP', \n",
    "          \"no_heuristic\":False, \"non_linearity\":'relu', \"num_layers\":4, \"num_mixtures\":6, \"num_nodes\":1, \"output_conv\":True, \"output_dim\":2, \n",
    "          \"output_prediction\":True, \"precision\":32, \"predict_delta\":False, \"resume_from_checkpoint\":None, \n",
    "          \"scheduler_step_size\":[60, 90, 120, 150, 180], \"seed\":None, \"segment_CL\":False, \"segment_CL_Encoder\":False, \n",
    "          \"segment_CL_Encoder_Gaussian\":False, \"segment_CL_Encoder_Gaussian_Prob\":False, \"segment_CL_Encoder_Prob\":True, \n",
    "          \"segment_CL_Gaussian_Prob\":False, \"segment_CL_Prob\":False, \"use_centerline_features\":True, \"use_oracle\":False, \"waypoint_step\":5, \n",
    "          \"weight_decay\":0.0, \"workers\":8, \"wta\":False, \"draw_image\" : True, \"remove_high_related_score\" : True, \"maximum_delete_num\" : 3, \n",
    "       \"is_LRP\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f6cda4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:59:40.382645Z",
     "start_time": "2022-02-17T18:59:40.002895Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = ArgoverseDataset(\n",
    "    args[\"dataroot\"],\n",
    "    mode=\"train\",\n",
    "    delta=args[\"predict_delta\"],\n",
    "    map_features_flag=args[\"map_features\"],\n",
    "    social_features_flag=True,\n",
    "    heuristic=(not args[\"no_heuristic\"]),\n",
    "    ifc=args[\"IFC\"],\n",
    "    is_oracle=args[\"use_oracle\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207ecea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:59:44.072626Z",
     "start_time": "2022-02-17T18:59:43.543064Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_loader = ArgoverseDataset(\n",
    "    args[\"dataroot\"],\n",
    "    mode=\"train\",\n",
    "    delta=args[\"predict_delta\"],\n",
    "    map_features_flag=args[\"map_features\"],\n",
    "    social_features_flag=True,\n",
    "    heuristic=(not args[\"no_heuristic\"]),\n",
    "    ifc=args[\"IFC\"],\n",
    "    is_oracle=args[\"use_oracle\"],\n",
    ")\n",
    "\n",
    "val_loader = ArgoverseDataset(\n",
    "    args[\"dataroot\"],\n",
    "    mode=\"val\",\n",
    "    delta=args[\"predict_delta\"],\n",
    "    map_features_flag=args[\"map_features\"],\n",
    "    social_features_flag=True,\n",
    "    heuristic=(not args[\"no_heuristic\"]),\n",
    "    ifc=args[\"IFC\"],\n",
    "    is_oracle=args[\"use_oracle\"],\n",
    ")\n",
    "\n",
    "test_loader = ArgoverseDataset(\n",
    "    args[\"dataroot\"],\n",
    "    mode=\"test\",\n",
    "    delta=args[\"predict_delta\"],\n",
    "    map_features_flag=args[\"map_features\"],\n",
    "    social_features_flag=True,\n",
    "    heuristic=(not args[\"no_heuristic\"]),\n",
    "    ifc=args[\"IFC\"],\n",
    "    is_oracle=args[\"use_oracle\"],\n",
    ")\n",
    "\n",
    "train_dataset = DataLoader(\n",
    "    train_loader,\n",
    "    batch_size=args[\"batch_size\"],\n",
    "    num_workers=args[\"workers\"],\n",
    "    pin_memory=True,\n",
    "    collate_fn=ArgoverseDataset.collate,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dataset = DataLoader(\n",
    "    val_loader,\n",
    "    batch_size=args[\"batch_size\"],\n",
    "    num_workers=args[\"workers\"],\n",
    "    pin_memory=True,\n",
    "    collate_fn=ArgoverseDataset.collate,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "trest_dataset = DataLoader(\n",
    "    test_loader,\n",
    "    batch_size=args[\"batch_size\"],\n",
    "    num_workers=args[\"workers\"],\n",
    "    pin_memory=True,\n",
    "    collate_fn=ArgoverseDataset.collate,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "# dm = ArgoverseDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8421858f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T18:59:47.354797Z",
     "start_time": "2022-02-17T18:59:47.352446Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for d in val_dataset:\n",
    "#     print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37118c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:00:01.443874Z",
     "start_time": "2022-02-17T18:59:55.273069Z"
    }
   },
   "outputs": [],
   "source": [
    "model = WIMP(args)\n",
    "model.load_state_dict(torch.load(\"experiments/example_old/checkpoints/epoch=122.ckpt\")['state_dict'])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d506f11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:00:01.449362Z",
     "start_time": "2022-02-17T19:00:01.446187Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metric(metric_dict, ade,fde,mr,loss):\n",
    "    metric_dict[\"ade\"] += ade\n",
    "    metric_dict[\"fde\"] += fde\n",
    "    metric_dict[\"mr\"] += mr\n",
    "    metric_dict[\"loss\"] += loss\n",
    "    metric_dict[\"length\"]+=1\n",
    "\n",
    "# a = torch.FloatTensor([1])\n",
    "# get_metric(origina_model_metric, a,a,a,a)\n",
    "# origina_model_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dae5442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:00:09.054363Z",
     "start_time": "2022-02-17T19:00:09.044044Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  optimizer는 선언하지 않았습니다.\n",
    "#  따라서 아마 model weight에 gradient는 계속 쌓이겠지만 저희가 중요한 것은 adjacency matrix의 graident이므로 상관 없을 것으로 예측됩니다.\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "Relu = nn.ReLU()\n",
    "\n",
    "save_foler = \"ResultsImg/\"\n",
    "\n",
    "save_XAI = save_foler + \"/XAI/\"\n",
    "save_attention = save_foler + \"/attention\"\n",
    "\n",
    "slicing = lambda a, idx: torch.cat((a[:, :idx], a[:, idx+1:]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83108ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:03:47.537277Z",
     "start_time": "2022-02-17T19:03:47.532975Z"
    }
   },
   "outputs": [],
   "source": [
    "import XAI_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd1b17a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:08:43.935761Z",
     "start_time": "2022-02-17T19:08:39.555903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/39472 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/39472 [00:01<21:30:24,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/39472 [00:03<16:27:21,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-39.4454, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/39472 [00:04<23:31:30,  2.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-454fe3054e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         preds, waypoint_preds, all_dist_params, att_weights, adjacency, _, _ = model(input_dict[\"agent_features\"],\n\u001b[0m\u001b[1;32m    114\u001b[0m                                                     \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"social_features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                                                     \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adjacency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/MotionPrediction/WIMP/src/models/WIMP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, agent_features, social_features, adjacency, num_agent_mask, outsteps, social_label_features, label_adjacency, classmate_forcing, labels, ifc_helpers, test, map_estimate, gt, idx, sample_next, num_predictions, am)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxy_kernel_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mlast_n_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mprediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaypoints_prediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_n_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifc_helpers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mifc_helpers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_next\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_estimate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_estimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwaypoints_prediction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaypoint_predictions_tensor_encoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/MotionPrediction/WIMP/src/models/WIMP_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_input_features, last_n_predictions, hidden_decoder, outsteps, ifc_helpers, sample_next, map_estimate, mixture_num, sample_centerline)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Waypoint Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mcurr_waypoint_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mcurr_waypoint_decoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_waypoint_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaypoint_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_waypoint_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "abs_min = lambda weight : torch.argmin(abs(weight)[1:]).item()\n",
    "abs_max = lambda weight : torch.argmax(abs(weight)[1:]).item()\n",
    "simple_min = lambda weight : torch.argmin(weight[1:]).item()\n",
    "simple_max = lambda weight : torch.argmax(weight[1:]).item()\n",
    "\n",
    "\n",
    "names = [\"abs_min\", \"abs_max\", \"simple_min\",\"simple_max\"]\n",
    "for name_idx, function in enumerate([abs_min, abs_max, simple_min, simple_max]):\n",
    "    origina_model_metric  ={\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "        \"length\" : 0\n",
    "    }\n",
    "\n",
    "    DA_model_metric = [\n",
    "        {\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"length\" : 0    \n",
    "        },\n",
    "        {\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"length\" : 0    \n",
    "        },\n",
    "        {\n",
    "            \"ade\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"fde\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"mr\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"loss\" : torch.FloatTensor([0]).cuda(),\n",
    "            \"length\" : 0\n",
    "        },\n",
    "    ] # 하나씩 지우면서 metric을 잴것임\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(val_dataset)):\n",
    "        input_dict, target_dict = batch[0], batch[1]\n",
    "\n",
    "        # get cuda\n",
    "        input_dict['agent_features'] = input_dict['agent_features'].cuda()\n",
    "        input_dict['social_features'] = input_dict['social_features'].cuda()\n",
    "        input_dict['social_label_features'] = input_dict['social_label_features'].cuda()\n",
    "        input_dict['adjacency'] = input_dict['adjacency'].cuda()\n",
    "        input_dict['label_adjacency'] = input_dict['label_adjacency'].cuda()\n",
    "        input_dict['num_agent_mask'] = input_dict['num_agent_mask'].cuda()\n",
    "        input_dict['ifc_helpers']['agent_oracle_centerline'] = input_dict['ifc_helpers']['agent_oracle_centerline'].cuda()\n",
    "        input_dict['ifc_helpers']['agent_oracle_centerline_lengths'] = input_dict['ifc_helpers']['agent_oracle_centerline_lengths'].cuda()\n",
    "        input_dict['ifc_helpers']['social_oracle_centerline'] = input_dict['ifc_helpers']['social_oracle_centerline'].cuda()\n",
    "        input_dict['ifc_helpers']['social_oracle_centerline_lengths'] = input_dict['ifc_helpers']['social_oracle_centerline_lengths'].cuda()\n",
    "        input_dict['ifc_helpers']['agent_oracle_centerline'] = input_dict['ifc_helpers']['agent_oracle_centerline'].cuda()\n",
    "        target_dict['agent_labels'] = target_dict['agent_labels'].cuda()\n",
    "\n",
    "\n",
    "        preds, waypoint_preds, all_dist_params, attention, adjacency, _, _ = model(**input_dict)\n",
    "        loss, (ade, fde, mr) = model.eval_preds(preds, target_dict, waypoint_preds)\n",
    "        get_metric(origina_model_metric, ade,fde,mr,loss)\n",
    "\n",
    "\n",
    "        input_dict['adjacency'].requires_grad = True\n",
    "        input_dict['adjacency'].retain_grad()\n",
    "\n",
    "        adjacency.retain_grad()\n",
    "        print(torch.sum(model.decoder.value_generator.weight))\n",
    "        loss.backward()\n",
    "        print(torch.sum(model.decoder.value_generator.weight))\n",
    "        batch_preds = preds\n",
    "\n",
    "\n",
    "\n",
    "        for idx in range(args[\"batch_size\"]):\n",
    "            if args[\"draw_image\"]:\n",
    "                weight = adjacency.grad[idx][0].cpu().numpy()\n",
    "                att = attention[idx].cpu().numpy()\n",
    "                agent_features = input_dict['agent_features'][idx].cpu().numpy()\n",
    "                social_features = input_dict['social_features'][idx].cpu().numpy()\n",
    "                # target = target_dict['agent_labels'][idx].cpu().numpy()\n",
    "                preds = batch_preds[idx][:,:,:,:2][0].cpu().detach().numpy()\n",
    "                city_name = input_dict['ifc_helpers']['city'][idx]\n",
    "                rotation = input_dict['ifc_helpers']['rotation'][idx].numpy()\n",
    "                translation = input_dict['ifc_helpers']['translation'][idx].numpy()\n",
    "#                 XAI_utils.draw_attention(agent_features, social_features, preds, city_name, rotation, translation, \n",
    "#                                                weight = copy.deepcopy(att), draw_future=True, save_fig = True, \n",
    "#                                                  save_name= save_attention + \"/\" + str(batch_idx) + \"_\" + str(idx) + \".png\")\n",
    "\n",
    "                XAI_utils.draw(agent_features, social_features, preds, city_name, rotation, translation, \n",
    "                                   weight = copy.deepcopy(weight), draw_future=True, save_fig = True, \n",
    "                                   save_name = save_XAI + \"/\" + str(batch_idx) + \"_\" + str(idx) + \".png\")\n",
    "\n",
    "            if args[\"remove_high_related_score\"]:\n",
    "                weight = adjacency.grad[idx][0]\n",
    "                for i in range(args[\"maximum_delete_num\"]):\n",
    "    #                 print(batch_idx, input_dict[\"social_features\"].shape)\n",
    "                    if len(input_dict[\"social_features\"][0] > 1):\n",
    "                        arg = function(weight)\n",
    "    #                     arg_max = torch.argmin(weight[1:]).item()\n",
    "\n",
    "                        weight = torch.cat((weight[:arg+1], weight[arg+2:]))                                                                                                                                                                                                                                                                                                                                                                             \n",
    "                        input_dict[\"social_features\"]                                   = slicing(input_dict[\"social_features\"], arg)\n",
    "                        input_dict[\"num_agent_mask\"]                               = slicing(input_dict[\"num_agent_mask\"], arg+1)\n",
    "                        input_dict[\"ifc_helpers\"]['social_oracle_centerline']= slicing(input_dict[\"ifc_helpers\"]['social_oracle_centerline'], arg)\n",
    "                        input_dict[\"ifc_helpers\"]['social_oracle_centerline_lengths']  = slicing(input_dict[\"ifc_helpers\"]['social_oracle_centerline_lengths'], arg)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        preds, waypoint_preds, all_dist_params, att_weights, adjacency, _, _ = model(input_dict[\"agent_features\"],\n",
    "                                                    input_dict[\"social_features\"],\n",
    "                                                    input_dict['adjacency'],\n",
    "                                                    input_dict[\"num_agent_mask\"],\n",
    "                                                    ifc_helpers ={\n",
    "                                                        \"social_oracle_centerline\": input_dict[\"ifc_helpers\"]['social_oracle_centerline'], \n",
    "                                                        \"social_oracle_centerline_lengths\": input_dict[\"ifc_helpers\"]['social_oracle_centerline_lengths'],\n",
    "                                                        \"agent_oracle_centerline\": input_dict[\"ifc_helpers\"][\"agent_oracle_centerline\"],\n",
    "                                                        \"agent_oracle_centerline_lengths\": input_dict[\"ifc_helpers\"][\"agent_oracle_centerline_lengths\"]\n",
    "                                                    })\n",
    "                        loss, (ade, fde, mr) = model.eval_preds(preds, target_dict, waypoint_preds)\n",
    "                        get_metric(DA_model_metric[i], ade,fde,mr,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f69ad26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:10:01.579852Z",
     "start_time": "2022-02-17T19:10:01.572938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResultsImg//XAI//2_0.png'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_XAI + \"/\" + str(batch_idx) + \"_\" + str(idx) + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98490830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T19:10:08.676828Z",
     "start_time": "2022-02-17T19:10:08.232864Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp 'ResultsImg//XAI//2_0.png' ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fb52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T01:05:12.166915Z",
     "start_time": "2022-02-14T01:04:58.658Z"
    }
   },
   "outputs": [],
   "source": [
    "model = WIMP(args)\n",
    "model.load_state_dict(torch.load(\"experiments/example/checkpoints/epoch=122.ckpt\")['state_dict'])\n",
    "model.cuda()\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3bece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T01:05:12.167617Z",
     "start_time": "2022-02-14T01:04:58.659Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "preds, waypoint_preds, all_dist_params, att_weights, adjacency = model(**input_dict)\n",
    "loss, (ade, fde, mr) = model.eval_preds(preds, target_dict, waypoint_preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732a5f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T01:05:12.168680Z",
     "start_time": "2022-02-14T01:04:58.660Z"
    }
   },
   "outputs": [],
   "source": [
    "model.encoder.xy_conv_filters[0].weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f1797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
